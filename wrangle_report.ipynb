{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA WRANGLING REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By Jerome Gameli Kofi Davor, Data Analyst (Udacity Nanodegree Program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Objective: \n",
    "\n",
    "To effectively wrangle data related to the [@WeRateDogs](https://twitter.com/dog_rates) twitter account and use result for analysis and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Details: \n",
    "\n",
    "The processes for this project are:\n",
    "\n",
    "- Data Gathering\n",
    "- Data Assessment \n",
    "- Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different datasets that were gathered as follows:\n",
    "\n",
    "__Twitter Archive File__: A **`twitter_archive_enhanced.csv`** file was provided in the project workspace by Udacity and was downloaded to my local workspace in jupyter notebook. I used python's pandas library to load and read the file into a dataframe named **`twitter_archive`**.\n",
    "\n",
    "__Tweet image prediction file__: I used Python requests and os functions which I imported into the project workspace form the python requests and os libraries. With the `get()` function of the requests library, I got the data through its url and saved it in a response variable. Response displayed a status code of 200, meaning request was successful. Using the with `open()` function, the response’s content was written to a **`image_predictions.tsv`** file, which was loaded and read into a dataframe called **`image_predict`**.\n",
    "\n",
    "__Tweet_Json text file__: For this file, I created a twitter developer account, created an application and used generated app credentials (consumer_key, consumer_secret, access_toke, and access_secret) for twitter API authentication. I then imported tweepy and json libraries, authenticated `tweepy.OAuthHandler` and `set wait_on_limit` parameter to **`True`**, in order to continue after each waiting time. I also used the tweet id to scrape more data online, created an empty dictionary to save failed tweets and set up a timer for the whole process. . A **`tweet_json.txt`** file was created and the output written into it, failed ones were appended to the empty dictionary and time taken and the failed dictionary printed.\n",
    "With a `for loop`, tweet_json.txt was read line by line into a json file and converted to a dataframe named **`tweet_json`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets were assessed as follows:\n",
    "\n",
    "**Visually**: The dataframes were displayed in a jupyter notebook and scrolled through to make a visual assessments. I used excel to visually assess the csv files.\n",
    "\n",
    "**Programmatically**: I did various programmatic assessment with pandas methods and functions such as `.info()`, `.describe()`, `.isnull()`, `.head()`, `.sample()`, `.duplicated()`, `.value_counts()` and `shape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stage was in three parts:\n",
    "1. Define\n",
    "2. Code\n",
    "3. Test.\n",
    "\n",
    "A copy of the original datasets were made to ensure data loss or mistakes did not affect our original datasets.\n",
    "\n",
    "- twitter_archive = df_1_clean \n",
    "- image_predict = df_2_clean \n",
    "- tweet_json = df_3_clean\n",
    "\n",
    "I followed the Define, Code and Test processes for the following cleaning efforts:\n",
    "\n",
    "- Removed retweets that won’t be used for analysis.\n",
    "\n",
    "\n",
    "- Dropped retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id and in_reply_to_user_id columns because they had 90% of values missing.\n",
    "\n",
    "\n",
    "- Merged the four dog stages columns into one column.\n",
    "\n",
    "\n",
    "- Dropped followers_count and friends_count columns as they won't be relevant to the analysis.\n",
    "\n",
    "\n",
    "- Converted the timestamp to datetime.\n",
    "\n",
    "\n",
    "- Converted the tweet_ids to string.\n",
    "\n",
    "\n",
    "- Dropped all values in the name column that started with small letters because they weren’t dog names.\n",
    "\n",
    "\n",
    "- Changed all p1, p2, and p3 values to lower case.\n",
    "\n",
    "\n",
    "- Changed column label from 'id' to 'tweet_id' in tweet_json.\n",
    "\n",
    "\n",
    "- Merged the three dataframes into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged data was saved into a csv file named **`twitter_archive_master.csv`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data wrangling project was both challenging and fun for me. I was able to sharpen my skills in assessing data programmatically using python packages and libraries, document issues and clean the data to successfully gain insights from them through visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
